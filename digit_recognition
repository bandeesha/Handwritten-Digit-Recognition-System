import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')

# View files in the input directory
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Load data
train = pd.read_csv('train.csv')
#test = pd.read_csv('test.csv')

# Display data samples
print(train.head())
print(train.tail())
#print(test.head())
#print(test.tail())

# Info and description
print(train.info())
#print(test.info())
print(train.describe().T)
#print(test.describe().T)

# Visualize label distribution
sns.set_theme(style="whitegrid")
colors = sns.color_palette("viridis", len(train['label'].unique()))

plt.figure(figsize=(10, 6))
ax = sns.countplot(x=train['label'], palette=colors, order=sorted(train['label'].unique()))
plt.title("Distribution of Digits in Training Set")
plt.show()

# Show some sample digits
fig, axes = plt.subplots(1, 5, figsize=(15, 5), constrained_layout=True)
for i, ax in enumerate(axes):
    ax.imshow(train.iloc[i, 1:].values.reshape(28, 28), cmap='gray')
    ax.set_title(f"Label: {train['label'][i]}", fontsize=12, fontweight='bold', color='darkblue')
    ax.axis('off')
fig.suptitle("Sample Digits from the Dataset", fontsize=16, fontweight='bold', color='darkgreen')
plt.show()

# Preprocess data
X = train.drop('label', axis=1).values / 255.0
y = train['label'].values
X = X.reshape(-1, 28, 28, 1)
y = tf.keras.utils.to_categorical(y, 10)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training data shape: {X_train.shape}")
print(f"Validation data shape: {X_val.shape}")

# Define CNN model
model = Sequential([
    Input(shape=(28, 28, 1)),
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.3),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.4),

    Conv2D(256, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.5),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

model.summary()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=12,
    zoom_range=0.15,
    width_shift_range=0.15,
    height_shift_range=0.15
)
datagen.fit(X_train)

# Visualize augmented images
images, labels = next(datagen.flow(X_train, y_train, batch_size=9))
plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
plt.show()

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)

# Train the model
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=64),
    validation_data=(X_val, y_val),
    epochs=30,
    callbacks=[early_stopping, reduce_lr]
)

# Plot training history
fig, axes = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)
axes[0].plot(history.history['accuracy'], label='Train Accuracy', color='dodgerblue', marker='o')
axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange', linestyle='--', marker='s')
axes[0].set_title('Model Accuracy', fontsize=14)
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('Accuracy')
axes[0].legend()
axes[0].grid(True)

axes[1].plot(history.history['loss'], label='Train Loss', color='green', marker='x')
axes[1].plot(history.history['val_loss'], label='Validation Loss', color='red', linestyle='--', marker='^')
axes[1].set_title('Model Loss', fontsize=14)
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Loss')
axes[1].legend()
axes[1].grid(True)
plt.show()

# Evaluate on validation set
val_loss, val_accuracy = model.evaluate(X_val, y_val)
print(f"Validation Accuracy: {val_accuracy:.4f}")

# Predict on test data
test = pd.read_csv('test.csv').values
test_normalized = test / 255.0
test_normalized = test_normalized.reshape(-1, 28, 28, 1)
predictions = np.argmax(model.predict(test_normalized), axis=1)

# Create submission file
submission = pd.DataFrame({'ImageId': range(1, len(predictions) + 1), 'Label': predictions})
submission.to_csv('submission.csv', index=False)
print("Submission file saved as 'submission.csv'")
